{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6dfe06",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba58532",
   "metadata": {},
   "source": [
    "## 目标\n",
    "1. 想考虑更多的语意信息\n",
    "2. 不要把所有信息压缩到一个隐藏层里，逐步融合所有信息\n",
    "3. 参考WaveNet论文"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0298e2c7",
   "metadata": {},
   "source": [
    "pytorch@这个运算符很强大，(2,4,5,8)@(8,200)依然可以运算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee6cc1c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "steb1. 将embedding veiw封装到类，进而可以调用时装入layer的列表\n",
    "step2. container概念，（a way of organizing layers into lists or dicts, 即写一个sequential类,传入参数是layer的列表等，前向传播时就可以直接model(x)\n",
    "step3. （pytorch@这个运算符很强大，(2,4,5,8)@(8,200)依然可以运算）每一层只拼接两个，分多层变换，直到将三位张量变为二维张量\n",
    "（这里训练发现和原来结果差不多,karpathy进行了两点反思：第一从架构层面上，他说实际上并没有对架构做过多的调整，在如何将参数分配给各层方面，还有很多参数搜寻可以做。第二，反思深入具体的原理，注意到这里处理的三位张量，BN依然只会处理第一维度，检查到batchnorm     ![image.png](https://img.krisonzhang.cn/img/20250503152800266.png)进行了这里的debug,发现形状理想状态应该是仅仅68，而非二维。\n",
    "于是想办法求均值是处理前两维而非仅仅一维。有注意到torch.mean不仅仅可以传入指定维度，还可以传入元组进行操作\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc46d1",
   "metadata": {},
   "source": [
    "注意Xtr[7] 是一个样本，\n",
    "Xtr[[7]] 是一个 batch，batch size 为 1。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c618b1",
   "metadata": {},
   "source": [
    "挖坑：\n",
    "1. 卷积操作\n",
    "2. 代码的复用，在juypnb中积累代码，开发中经常复制粘贴，不用从头考虑形状\n",
    "3. 更pytorch话，规范化"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
