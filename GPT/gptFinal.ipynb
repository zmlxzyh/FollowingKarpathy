{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7a58101",
   "metadata": {},
   "source": [
    "## 位置编码\n",
    "在attention is all you need论文中，位置嵌入实际上被初始化为不同频率的正弦和余弦，是固定的。但是在GPT2,这些只是参数，它们像其他任何参数一样从头训练。效果似乎差不多，所以可视化的结果显示在优化过程中有点像"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd28ae",
   "metadata": {},
   "source": [
    "## gpt2架构\n",
    "gpt2是对原始transformer修改后的版本，没有解码器，只有解码器，交叉注意力的部分也缺失了， \n",
    "与原始论文相比，层归一化改变了顺序，(moved to the input of each sub-block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db884de7",
   "metadata": {},
   "source": [
    "attention是一个聚合函数，是一个池化函数，一个加权函数，一个reduce operation.\n",
    "attention是reduce,mlp是map(没有交流，仅仅映射)，transformer就是reduce-map不断重复的过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c47d3",
   "metadata": {},
   "source": [
    "## gelu(gaussian error linear unit)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
